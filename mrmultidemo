#!/usr/bin/env python3

import sys
import os
from multiprocessing import Pool, JoinableQueue, Process
import itertools
import threading
import re
import time
from random import randint

def Map(L):
  """
  Given a list of tokens, return a list of tuples of
  titlecased (or proper noun) tokens and a count of '1'.
  Also remove any leading or trailing punctuation from
  each token.
  """
  results = []
  for w in L:
    # True if w contains non-alphanumeric characters
    if not w.isalnum():
      w = sanitize(w)
    # True if w is a title-cased token
    if w.istitle():
      results.append ((w, 1))
  return results


def Partition(L):
  """
  Group the sublists of (token, 1) pairs into a term-frequency-list
  map, so that the Reduce operation later can work on sorted
  term counts. The returned result is a dictionary with the structure
  {token : [(token, 1), ...] .. }
  """
  tf = {}
  for sublist in L:
    for p in sublist:
      # Append the tuple to the list in the map
      try:
        tf[p[0]].append (p)
      except KeyError:
        tf[p[0]] = [p]
  return tf



def Reduce(Mapping):
  """
  Given a (token, [(token, 1) ...]) tuple, collapse all the
  count tuples from the Map operation into a single term frequency
  number for this token, and return a final tuple (token, frequency).
  """
  return (Mapping[0], sum(pair[1] for pair in Mapping[1]))


def sanitize(w):
  """
  If a token has been identified to contain
  non-alphanumeric characters, such as punctuation,
  assume it is leading or trailing punctuation
  and trim them off. Other internal punctuation
  is left intact.
  """
  # Strip punctuation from the front
  while len(w) > 0 and not w[0].isalnum():
    w = w[1:]
  # String punctuation from the back
  while len(w) > 0 and not w[-1].isalnum():
    w = w[:-1]
  return w


def load(path):
  """
  Load the contents the file at the given
  path into a big string and return it.
  """
  word_list = []
  with open(path, "r") as f:
    for line in f:
      word_list.append (line)
  # Efficiently concatenate Python string objects
  return (''.join(word_list)).split()


def chunks(l, n):
  """
  I am a generator function for chopping up a given list into chunks of
  length n.
  """
  for i in range(0, len(l), n):
    yield l[i:i+n]


def tuple_sort(a, b):
  """
  Sort tuples by term frequency, and then alphabetically.
  """
  if a[1] < b[1]:
    return 1
  elif a[1] > b[1]:
    return -1
  else:
    return cmp(a[0], b[0])

########################################################




class FolderTupleGenerator(object):
  """
  I instantiate an Iterator, which performs lazy evaluation
  of the os.walk call on the concatenation of topfolders.
  My 'next' nethod returns a tuple (path,dirlist,filelist)
  for exactly one walked folder path.
  """
  def __init__(self,topfolders):
    self.topfolders = topfolders
    print("FolderTupleGenerator has topfolders %r" % (self.topfolders))
    self.iterator = itertools.chain.from_iterable([os.walk(d) for d in topfolders])

  def __iter__(self):
    return self

  def __next__(self):
    return self.next()

  def next(self):
    try:
      item = list(self.iterator.__next__())
    except StopIteration:
      # print("FolderTupleGenerator caught and re-raising StopIteration")
      raise StopIteration
    # print("FolderTupleGenerator.next -> %r" % (item))
    return item


class PathGenerator(object):
  """
  I instantiate an iterator, which performs lazy evaluation of
  the os.walk call.
  My 'next' method returns one full pathname from my current dtuple,
  until it is exhausted, in which case I call foldertuplegenerator.next()
  to obtain a new dtuple for the next folder in the os.walk.
  """
  def __init__(self,foldertuplegenerator):
    self.foldertuplegenerator = foldertuplegenerator
    self.dtuple = None

  def __iter__(self):
    return self

  def __next__(self):
    return self.next()

  def next(self):
    if self.dtuple is None or len(self.dtuple[2]) == 0:  # no paths left, get some more
      # print("PathGenerator out of dtuple.filelist, obtain another")
      try:
        self.dtuple = self.foldertuplegenerator.next()
      except StopIteration:
        raise StopIteration
   
    filename = self.dtuple[2].pop()
    pathname = os.path.join(self.dtuple[0],filename)
    # print("PathGenerator -> %r" % (pathname))
    return pathname


class ChunkedPayloadGenerator(object):
  """
  I instantiate an Iterator, which performs lazy evaluation and assembly of
  a 'payload', which is an enumerated (non-lazy) list of pathnames.
  """

  def __init__(self,payloadgenerator,payloadmax):
    self.payloadgenerator = payloadgenerator
    self.payloadmax        = payloadmax

  def __iter__(self):
    return self

  def __next__(self):
    return self.next()

  def next(self):
    payload = []
    for n in range(self.payloadmax):
      try:
        item = self.payloadgenerator.next()
        payload.append(item)
        # print("ChunkedPayloadGenerator got %s, len(payload) is %s" % (item,len(payload)))
      except StopIteration:
        # print("ChunkedPayloadGenerator caught StopIteration, --> payload is %r" % (payload))
        break
    if len(payload) > 0:
      return payload
    else:
      raise StopIteration




def do_work(item):
  print("Thread %r item %r started" % (threading.current_thread().name,item))
  time.sleep(randint(1,5))
  print("Thread %r item %r completed" % (threading.current_thread().name,item))


def worker():
  while True:
    item = q.get()
    if item is not None: do_work(item)
    q.task_done()




if __name__ == '__main__':

  # Build a pool of 8 processes
  pool = Pool(processes=8,)

  # Construct an interator for file-finding in the target_folders,
  # returns a lazy list of pathnames.
  target_folders = ['./testfolder1','testfolder2']
  foldertuplegenerator = FolderTupleGenerator(target_folders)
  pathgenerator = PathGenerator(foldertuplegenerator)
  chunkedpayloadgenerator = ChunkedPayloadGenerator(pathgenerator,4)

  for p in chunkedpayloadgenerator:
    print("p is %r" % (p))



  num_worker_threads = 5
  num_worker_processes = 5

  lorem =\
"""
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras congue pharetra leo, dignissim gravida ipsum rhoncus in. 
Integer diam felis, vulputate a nunc imperdiet, pretium vulputate felis. Donec euismod elit quam, 
vel adipiscing enim molestie non. Vivamus molestie velit et nunc consectetur, nec volutpat ipsum aliquet. 
Quisque rutrum lacus dictum, dignissim turpis consequat, facilisis justo. Proin hendrerit tellus a turpis accumsan, 
quis mattis massa eleifend. Integer non consectetur metus.
"""

  q = JoinableQueue()
  """
  for i in range(num_worker_threads):
    t = threading.Thread(target=worker,name="T%d" % (i))
    t.daemon = True
    t.start()
  """
  for i in range(num_worker_processes):
    p = Process(target=worker,name="T%d" % (i),daemon=True)
    p.start()

  for word in re.split("[\s,.]+",lorem):
    q.put(word)

  q.join()       # block until all tasks are done

  sys.exit(0)

  # Fragment the string data into 8 chunks
  partitioned_text = list(chunks(text, int(len(text) / 8)))

  # Generate count tuples for title-cased tokens
  single_count_tuples = pool.map(Map, partitioned_text)

  # Organize the count tuples; lists of tuples by token key
  token_to_tuples = Partition(single_count_tuples)

  # Collapse the lists of tuples into total term frequencies
  term_frequencies = pool.map(Reduce, token_to_tuples.items())
  print("term_frequencies is %r" % (term_frequencies))

  # Sort the term frequencies in nonincreasing order
  term_frequencies.sort(key=lambda x: x[1])

  for pair in term_frequencies[:20]:
    print(pair[0], ":", pair[1])


